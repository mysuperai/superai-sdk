{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "960e4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from superai.llm.dataset import Dataset, Data\n",
    "from superai.llm.data_types import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9cff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output schema\n",
    "input_schema = Any()\n",
    "output_schema = Any()\n",
    "\n",
    "# Create a dataset from a list of dictionaries\n",
    "data_list = [\n",
    "    {\"input\": \"What is the capital of France?\", \"output\": \"Paris\"},\n",
    "    {\"input\": \"What is the capital of Italy?\", \"output\": \"Rome\"},\n",
    "    {\"input\": \"What is the capital of Germany?\", \"output\": \"Berlin\"},\n",
    "    {\"input\": \"What is the capital of the USA?\", \"output\": \"Washington D.C.\"},\n",
    "    {\"input\": \"What is the capital of Austria?\", \"output\": \"Vienna\"},\n",
    "    {\"input\": \"What is the capital of Australia?\", \"output\": \"Canberra\"},\n",
    "    {\"input\": \"What is the capital of Japan?\", \"output\": \"Tokyo\"},\n",
    "    {\"input\": \"What is the capital of Scotland?\", \"output\": \"Edinburgh\"},\n",
    "    {\"input\": \"What is the capital of Spain?\", \"output\": \"Madrid\"},\n",
    "    {\"input\": \"What is the capital of the United Kingdom?\", \"output\": \"London\"},\n",
    "]\n",
    "\n",
    "# Create a dataset and add data to it\n",
    "data = [Data(input_value=i, output_value=o) for i,o in data_list]\n",
    "# for data in data_list:\n",
    "#     dataset.add_data(data[\"input\"], data[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "522d036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    \"train\": 0.7,\n",
    "    \"validation\": 0.2,\n",
    "    \"test\": 0.1\n",
    "}\n",
    "dataset = Dataset(input_schema=input_schema, output_schema=output_schema, data=data, splits=splits)\n",
    "# print(dataset.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c62dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 2, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset.get_split('train')\n",
    "validation_dataset = dataset.get_split('validation')\n",
    "test_dataset = dataset.get_split('test')\n",
    "len(train_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebd66fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.create_splits(test_size=0.5, train_size=0.2, validation_size=.3, stratify_by_column=None, seed=42)\n",
    "len(dataset.get_split('train')), len(dataset.get_split('validation')), len(dataset.get_split('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0778037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The full `train` split.\n",
    "# train_ds = Dataset(input_schema, output_schema, data, splits='train')\n",
    "\n",
    "# # The full `train` split and the full `test` split as two distinct datasets.\n",
    "# train_ds, test_ds = Dataset(input_schema, output_schema, data, splits=['train', 'test'])\n",
    "\n",
    "# # The full `train` and `test` splits, concatenated together.\n",
    "# train_test_ds = Dataset(input_schema, output_schema, data, splits='train+test')\n",
    "\n",
    "# # From record 10 (included) to record 20 (excluded) of `train` split.\n",
    "# train_10_20_ds = Dataset(input_schema, output_schema, data, splits='train[10:20]')\n",
    "\n",
    "# # The first 10% of `train` split.\n",
    "# train_10pct_ds = Dataset(input_schema, output_schema, data, splits='train[:10%]')\n",
    "\n",
    "# # The first 10% of `train` + the last 80% of `train`.\n",
    "# train_10_80pct_ds = Dataset(input_schema, output_schema, data, splits='train[:10%]+train[-80%:]')\n",
    "\n",
    "# # 10-fold cross-validation\n",
    "# vals_ds = Dataset(input_schema, output_schema, data, splits=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])\n",
    "# trains_ds = Dataset(input_schema, output_schema, data, splits=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4fbcb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scenario 1\n",
    "# train_ds = dataset.train\n",
    "# # Scenario 2\n",
    "# train_ds, test_ds = dataset.train, dataset.test\n",
    "# # Scenario 3\n",
    "# combined_ds = dataset.train + dataset.test\n",
    "# # Scenario 4\n",
    "# train_subset_1 = dataset.train[10:20]\n",
    "# # Scenario 5\n",
    "# train_subset_2 = dataset.train[:'10%']\n",
    "# # Scenario 6\n",
    "# combined_subset = dataset.train[:'10%'] + dataset.test[:'-80%']\n",
    "\n",
    "# # Additional Scenarios\n",
    "# val_ds = [dataset[f'train[{k}%:{k+10}%]'] for k in range(0, 100, 10)]\n",
    "# train_ds = [dataset[f'train[:{k}%]+train[{k+10}%:]'] for k in range(0, 100, 10)]\n",
    "\n",
    "# train_subset_3 = dataset.train['50%:52%']\n",
    "# train_subset_4 = dataset.train['52%:54%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e2e7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add data to the dataset\n",
    "# new_data = {\"input\": \"What is the capital of Spain?\", \"output\": \"Madrid\"}\n",
    "# dataset.add_data(input=new_data[\"input\"], output=new_data[\"output\"])\n",
    "\n",
    "# # Update data in the dataset\n",
    "# update_index = 1\n",
    "# updated_data = {\"output\": \"Roma\"}\n",
    "# dataset.update_data(index=update_index, output=updated_data[\"output\"])\n",
    "\n",
    "# # Delete data from the dataset\n",
    "# delete_index = 0\n",
    "# dataset.delete_data(delete_index)\n",
    "\n",
    "# # Read data from the dataset\n",
    "# read_index = 1\n",
    "# data = dataset[read_index]\n",
    "# print(data)\n",
    "\n",
    "# # Load dataset from JSON file\n",
    "# dataset = Dataset.from_json(\"my_file.json\", input_schema, output_schema)\n",
    "\n",
    "# # Save dataset to a JSON file\n",
    "# dataset.save_to_json(\"output.json\")\n",
    "\n",
    "# # Load dataset from a CSV file\n",
    "# dataset = Dataset.from_csv(\"my_file.csv\", input_schema, output_schema)\n",
    "\n",
    "# # Save dataset to a CSV file\n",
    "# dataset.save_to_csv(\"output.csv\")\n",
    "\n",
    "# # Load dataset from an SQLite database\n",
    "# dataset = Dataset.from_sql(\"SELECT * FROM table_name\", \"sqlite:///sqlite_file.db\", input_schema, output_schema)\n",
    "\n",
    "# # Save dataset to an SQLite database\n",
    "# dataset.save_to_sql(\"output_table\", \"sqlite:///sqlite_file.db\", if_exists=\"replace\")\n",
    "\n",
    "# # Convert dataset to a pandas DataFrame\n",
    "# df = dataset.to_pandas()\n",
    "\n",
    "# # Convert dataset to a dictionary\n",
    "# data_dict = dataset.to_dict()\n",
    "\n",
    "# # Convert dataset to a list\n",
    "# data_list = dataset.to_list()\n",
    "\n",
    "# # Convert dataset to a generator\n",
    "# generator = dataset.to_generator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The full `train` split.\n",
    "# train_ds = Data(data, split='train')\n",
    "# print(\"The full `train` split:\", train_ds.splits['train'])\n",
    "\n",
    "# # The full `train` split and the full `test` split as two distinct datasets.\n",
    "# train_ds, test_ds = load_dataset(data, split=['train', 'test'])\n",
    "# print(\"The full `train` split:\", train_ds.splits['train'])\n",
    "# print(\"The full `test` split:\", test_ds.splits['test'])\n",
    "\n",
    "# # The full `train` and `test` splits, concatenated together.\n",
    "# train_test_ds = load_dataset(data, split='train+test')\n",
    "# print(\"The full `train` and `test` splits, concatenated together:\", train_test_ds.splits['train+test'])\n",
    "\n",
    "# # From record 10 (included) to record 20 (excluded) of `train` split.\n",
    "# train_10_20_ds = load_dataset(data, split='train[10:20]')\n",
    "# print(\"From record 10 (included) to record 20 (excluded) of `train` split:\", train_10_20_ds.splits['train[10:20]'])\n",
    "\n",
    "# # The first 10% of `train` split.\n",
    "# train_10pct_ds = load_dataset(data, split='train[:10%]')\n",
    "# print(\"The first 10% of `train` split:\", train_10pct_ds.splits['train[:10%]'])\n",
    "\n",
    "# # The first 10% of `train` + the last 80% of `train`.\n",
    "# train_10_80pct_ds = load_dataset(data, split='train[:10%]+train[-80%:]')\n",
    "# print(\"The first 10% of `train` + the last 80% of `train`:\", train_10_80pct_ds.splits['train[:10%]+train[-80%:]'])\n",
    "\n",
    "# # 10-fold cross-validation\n",
    "# vals_ds = load_dataset(data, split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])\n",
    "# trains_ds = load_dataset(data, split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])\n",
    "\n",
    "# print(\"10-fold cross-validation:\")\n",
    "# for i in range(10):\n",
    "#     print(f\"Validation set {i}:\", vals_ds.splits[f'train[{i*10}%:{(i+1)*10}%]'])\n",
    "#     print(f\"Training set {i}:\", trains_ds.splits[f'train[:{i*10}%]+train[{(i+1)*10}%:]'])\n",
    "\n",
    "# # Percent slicing and rounding\n",
    "# train_50_52_ds = load_dataset(data, split='train[50%:52%]')\n",
    "# print(\"Assuming `train` split contains 999 records, 19 records from 500 (included) to 519 (excluded):\", train_50_52_ds.splits['train[50%:52%]'])\n",
    "\n",
    "# train_52_54_ds = load_dataset(data, split='train[52%:54%]')\n",
    "# print(\"Assuming `train` split contains 999 records, 20 records from 519 (included) to 539 (excluded):\", train_52_54_ds.splits['train[52%:54%]'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
