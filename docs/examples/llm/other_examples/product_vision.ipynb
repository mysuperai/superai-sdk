{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0636bc65",
   "metadata": {},
   "source": [
    "LLM Product vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca275a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import superai\n",
    "from superai.ai import AI \n",
    "from superai.dataset import Dataset\n",
    "from superai.evaluate import Evaluate\n",
    "from superai.prompt import Prompt\n",
    "from superai.application import Application, Workflow, Task\n",
    "from superai.models import ChatGPT\n",
    "from superai.datatypes import Document, Structured, SingleChoice, List, KeyValue, Code\n",
    "from superai.hub import notebook_login\n",
    "\n",
    "# 0 Install and authenticate\n",
    "!pip install superai \n",
    "!superai login #opens UI to login\n",
    "# or you can use notebook login\n",
    "from superai.hub import notebook_login\n",
    "notebook_login()\n",
    "# or you can use API key\n",
    "superai.apikey = \"sai-4n23n4r230r9420349fdjgd\"\n",
    "\n",
    "\n",
    "# 1 Choose an optional starting point from SuperAI.hub \n",
    "## load a data set \n",
    "dataset = Dataset.load(\"my_org/my_ai\")\n",
    "## load a prompt \n",
    "prompt = Prompt.load(\"my_org/my_prompt\")\n",
    "## load an AI \n",
    "ai = AI.load(\"my_org/my_ai\")\n",
    "## load an evaluation metric\n",
    "evaluation = Evaluate.load(\"my_org/my_metric\")\n",
    "## load an AI application\n",
    "application = Application.load(\"my_org/my_application\")\n",
    "\n",
    "# 2 Customize\n",
    "## add your custom data \n",
    "custom_dataset = Dataset(input_schema=Document(), output_schema=Structured())\n",
    "with open(documents_path, \"r\") as f:\n",
    "    documents = json.load(f)\n",
    "for i,document in enumerate(documents):\n",
    "    tables, key_value_pairs, fields = preprocess(document)\n",
    "    ground_truth = create_ground_truth(document, output_schema)\n",
    "    custom_dataset.add_data(input=tables, output=ground_truth)\n",
    "## create a custom Prompt \n",
    "custom_prompt = Prompt(prompt=\"test prompt\")\n",
    "## create a custom AI \n",
    "custom_ai = AI(input_schema=Document, output_schema=Structured(), prompt=custom_prompt, model=ChatGPT(engine='gpt-4', temperature=0))\n",
    "## optionally create an application (groups data, prompts, and AI and does quality control, labelling, etc.) (see https://dust.tt/spolu/a/01caf5ebc3, or https://drinkwater.ai/)\n",
    "custom_application = Application(input_schema=Document, output_schema=Structured())\n",
    "custom_application.set_constraints(constraints=[Accuracy()>.8, Seconds()<29.4, Dollars()<.3])\n",
    "custom_application.add_workers(workers=[Human(email=\"enrique@super.ai\"), AI.load(\"amazon/textract\"), Software.load(\"adobe/background_remover\")])\n",
    "custom_workflow = Workflow(input_schema=Document, output_schema=Structured())\n",
    "custom_task_1 = Task(input_schema=Document, output_schema=SingleChoice())\n",
    "custom_task_1.add_workers(workers=[Human(email=\"enrique@super.ai\"), AI.load(\"amazon/textract\"), Software.load(\"adobe/background_remover\")])\n",
    "custom_workflow.add_task(custom_task_1)\n",
    "custom_task_2 = Task(input_schema=List(value=[Document(),SingleChoice()]), output_schema=Structured())\n",
    "my_code = \"\"\"from library import function\n",
    "return function(input)\"\"\"\n",
    "custom_code = Code(value=my_code)\n",
    "custom_task_2.add_worker(custom_code)\n",
    "custom_workflow.add_task(custom_task_2)\n",
    "custom_application.add_workflow(workflow=my_workflow)\n",
    "### optionall label data\n",
    "for data in dataset:\n",
    "    labeled_data = custom_application.process(input=data.input, output=data.output) #it will use ground truth to help label\n",
    "### or you can do \n",
    "labeled_data = custom_application.process(data=dataset)\n",
    "## view in hybrid database (see https://docs.baseplate.ai/)\n",
    "## use synthetic data to test\n",
    "\n",
    "# 3a Train AI - Optimize Prompt (in-context learning)\n",
    "prompt_optimizer = AutoPrompt()\n",
    "## Customize optimization goals and constraints\n",
    "prompt_optimizer.set_training_constraints([Dollars()<200])\n",
    "prompt_optimizer.set_goals([Accuracy>.95, Dollars<.0001, Tokens()<8000])\n",
    "## Optimize prompt - Use different foundation models, different parameters of models, different context, different examples\n",
    "### behind the scenes it handles token limits, API limits, errors, iterates on training set \n",
    "### visualize progress and metrics \n",
    "### launches prompt playground UI to interact and give feedback\n",
    "optimized_prompts = prompt_optimizer.train(prompt=custom_prompt, n_prompts=3, max_iterations=100, train=training_data, validation=validation_data, test=test_data)\n",
    "## Add prompt to AI \n",
    "optimized_ai = AI.from_prompt(input_schmea=Document(), output_schema=Structured(), prompt=optimized_prompts[0])\n",
    "\n",
    "# 3b Train AI - Fine tune foundation Model\n",
    "## monitor progress, metrics, quality, cost, speed \n",
    "fine_tuned_ai = optimized_ai.finetune(data=custom_dataset)\n",
    "\n",
    "# 4 Evaluate \n",
    "predictions = fine_tuned_ai.predict(inputs=custom_dataset_test)\n",
    "## check final test score \n",
    "score = evaluate.compute(predictions=predictions.outputs, reference=custom_dataset_test.outputs)\n",
    "## compare different AI, parameters, prompts\n",
    "ai_1 = AI.from_prompt(input_schmea=Document(), output_schema=Structured(), prompt=optimized_prompts[1])\n",
    "ai_2 = AI.from_prompt(input_schmea=Document(), output_schema=Structured(), prompt=optimized_prompts[2])\n",
    "predictions_1 = ai_1.predict(inputs=custom_dataset_test)\n",
    "predictions_2 = ai_2.predict(inputs=custom_dataset_test)\n",
    "comparisons = evaluate.compute(reference=custom_dataset_test.outputs, predictions_1=predictions_1, predictions_2=predictions_2)\n",
    "## view dataset statistics \n",
    "stats = evaluate.compute(data=custom_dataset_test)\n",
    "## chat with AI and data \n",
    "response = dataset.chat(\"What are all the fields in these document?\")\n",
    "response = dataset[0].chat(\"What are all the fields in these document?\")\n",
    "response = fine_tuned_ai.chat(\"What are all the fields in these document?\")\n",
    "\n",
    "# 5 Deploy to hub\n",
    "fine_tuned_ai.save('my_org', name='my_ai', metadata=my_ai_metadata)\n",
    "optimized_prompts[0].save('my_org', name='my_prompt', metadata=my_prompt_metadata)\n",
    "labeled_data.save('my_org', name='my_data', metadata=my_dataset_metadata)\n",
    "custom_application.save('my_org', name='my_app', metadata=my_app_metadata)\n",
    "custom_evaluator.save('my_org', name='my_evaluator', metadata=my_evaluator_metadata)\n",
    "## view requests \n",
    "## track quality, cost, speed (look at docs.helicone.ai)\n",
    "## view versions\n",
    "## view user feedback (add feedback types and value)\n",
    "fine_tuned_ai.add_feedback(type=MultipleChoice(choices=[\"good\", \"bad\"]), value=\"bad\")\n",
    "## AB test (see https://docs.humanloop.com/docs/run-an-experiment)\n",
    "## Integration into other tools\n",
    "## Add AI to App \n",
    "## call api and log responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt builder \n",
    "\n",
    "## Prompt Components \n",
    "### Context \n",
    "#### prompt.add_context(Webpage(url=\"https://super.ai/documentation\"))\n",
    "#### prompt.add_context(Database(host=\"172.11.10.16\", port='8888', database=\"products\")\n",
    "## Describe task (system)\n",
    "## Add examples\n",
    "## Thoughts\n",
    "## Output format (structured, figma, webpage, youtube, Tiktok, Twitter, Image, Audio, Video, Text)\n",
    "## Negative prompt\n",
    "## Constraints \n",
    "## Input\n",
    "\n",
    "## prompt variables (into template {})\n",
    "\n",
    "## Change foundation models and parameters\n",
    "\n",
    "## Prompt history (show function call, restore)\n",
    "\n",
    "## Versions\n",
    "\n",
    "## AI optimized prompt \n",
    "### goals (shorter prompt, quality) \n",
    "### what has been optimized \n",
    "### diff comparison \n",
    "\n",
    "## Generate prompt (metaprompt)\n",
    "### Metaprompt \n",
    "### From examples (reverse prompting)\n",
    "### From similar prompt (train on prompts)\n",
    "### From Template \n",
    "### From components\n",
    "### From file \n",
    "\n",
    "## Prompt score \n",
    "### from ground truth \n",
    "### from user feedback\n",
    "### form customer feedback\n",
    "### Automatic score vs \"ideal prompt\" quality estimate\n",
    "\n",
    "## Prompt critique\n",
    "### User feedback \n",
    "### Chat with prompt\n",
    "### Missed examples \n",
    "\n",
    "## Prompt Constraints\n",
    "### Size (number of tokens)\n",
    "\n",
    "## Improve prompt\n",
    "### From AI (remove punctuation, fix spelling errors, shorten text)\n",
    "\n",
    "## Modify prompt \n",
    "### Change tone, length, complexity, topic, creativity\n",
    "### Generate variations that are more ___\n",
    "\n",
    "## Show premade values \n",
    "## Show similar prompts \n",
    "## Show choices for variables \n",
    "## Show prompt suggestion popups \n",
    "\n",
    "## Parameter sweeps\n",
    "### grid search of Variables \n",
    "### grid search of improvement functions\n",
    "\n",
    "## load, save, deploy\n",
    "\n",
    "## prompt query language (optional; lmql.ai)\n",
    "                   \n",
    "## Change foundation model and parameters\n",
    "\n",
    "## Show token and limit percent (https://platform.openai.com/tokenizer)\n",
    "\n",
    "## Prompt metadata\n",
    "### start seed to reproduce results\n",
    "\n",
    "## Add prompt structure \n",
    "### Add context, system instructions, etc. Each will have a prefix, postfix, and other parameters\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API tools \n",
    "               \n",
    "# Retries\n",
    "### {\"name\": \"retry-num\", \"description\":\"Number of retries\", \"default\":5}\n",
    "### {\"name\": \"retry-factor\", \"description\":\"Exponential backoff factor\", \"default\":2}\n",
    "### {\"name\": \"retry-min-timeout\", \"description\":\"Minimum timeout (in milliseconds) between retries\", \"default\":1000}\n",
    "### {\"name\": \"retry-max-timeout\", \"description\":\"Maximum timeout (in milliseconds) between retries\", \"default\":10000}\n",
    "\n",
    "# Caching\n",
    "## {\"name\": \"cache-enabled\", \"description\":\"enable storing and loading from your cache\", \"default\":True}\n",
    "## {\"name\": \"cache-read-only\", \"description\":\"treat your cache as a read-only\", \"default\":False}\n",
    "## {\"name\": \"cache-control\", \"description\":\"Allow you to configure based on the Cloudflare Cache Directive, currently only max-age\"}\n",
    "## {\"name\": \"cache-bucket-max-size\", \"description\":\"how large you want your Bucket size to be\", \"default\":20}\n",
    "\n",
    "# Token Limits \n",
    "### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef36083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual application builder (see console.magicflow.ai, n8n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
